<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Spot is powered by large language models to chain low-level skills to achieve language rearrangement tasks.">
  <meta name="keywords" content="Large Language Models, Open-vocabulary pick-and-place">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LSC: Language-guided Skill Coordination</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src=""></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <style>
    h2 {
      color: red;
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">LSC: Language-guided Skill Coordination</h1>
          <h1 class="title is-1 publication-title">for Open-Vocabulary Mobile Pick-and-Place</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sites.google.com/view/tyjimmyyang">Tsung-Yen Yang</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/SergioArnaud">Sergio Arnaud</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://languageguidedskillcoordination.github.io/">Kavit Shah</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="http://naoki.io/">Naoki Yokoyama</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=p463opcAAAAJ&hl=en">Alexander William Clegg</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.joannetruong.com/">Joanne Truong</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.ericundersander.com/">Eric Undersander</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.maksymets.com/">Oleksandr Maksymets</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://faculty.cc.gatech.edu/~sha9/">Sehoon Ha</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=DMTuJzAAAAAJ&hl=en">Mrinal Kalakrishnan</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://roozbehm.info/">Roozbeh Mottaghi</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://faculty.cc.gatech.edu/~dbatra/">Dhruv Batra</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://akshararai.github.io/">Akshara Rai</a><sup>1</sup></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Meta AI, FAIR Team</span>
            <span class="author-block"><sup>2</sup>Georgia Tech</span>
            <span class="author-block"><sup>*</sup>Equal Contribution</span>
          </div>
          <div class="publication_link">
            <span class="link-block">
              <a href="./static/pdf/CVPR_demo_LLMsForSpot_final_0613.pdf"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                </span>
                <span>Poster</span>
              </a>
            </span>
          </div>
          <br>
          <div class="is-size-5 publication-authors">
            <h2 class="author-block"></sup><b>CVPR Expo Meta AI Booth: <a href="https://hallerickson.ungerboeck.com/prod/app85.cshtml?aat=Zva%2bsPD7WeTWWGsGWvarzixed2IJHWExCd84dKNS6Yg%3d">Location</a>/Time: June 20, 21, 22 </b></h2>
          </div>
          <div class="is-size-5 publication-authors">
            <h2 class="author-block"></sup><b>CVPR Demo Track Booth: Location: Booth ID: 3, Demo ID: 44/Time: June 22 </b></h2>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Demo</h2>
        <br>
        <div class="videoWrapper">
          <!-- <iframe width="100%" height="480"
            src="" title="LSC" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>
          </iframe> -->
          <video id="replay-video"
                 autoplay
                 loop
                 controls
                 muted
                 preload
                 playsinline
                 width="100%">
            <source src="./static/videos/cvpr_video_1.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="content has-text-justified">
          <p>
            We demonstrate Language-guided Skill Coordination (LSC) for open-vocabulary mobile pick-and-place on Spot.
            A user provides a natural language instruction: <i>"Bring me the chocolates box, cereal box, and pill bottle, and put them on the bedroom table"</i>.
            And the robot navigates to the location of the target objects and places them on the room table.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- The first demo in the video shows a user typing <i>"find human and get mug and place it in the case"</i>,
LLMs take the text input and outputs a sequence of low-level skills:
<b>NavPolicy(<i>"human"</i>) -> PickPolicy(<i>"mug"</i>) -> NavPolicy(<i>"case"</i>) -> PlacePolicy()</b>.
When executing <b>NavPolicy(<i>"human"</i>)</b>,
a semantic feature map of the environment is queried to get a point-goal location of the human.
Then this point goal location is fed into a reinforcement-learning-trained navigation policy
to let Spot navigate to the target. After Spot finds the human,
<b>PickPolicy(<i>"mug"</i>)</b> is used to pick up the target object.
The pick policy is trained to move the arm to the target grasping location of the object
defined by the text so that Spot can grasp the object.
Finally, Spot uses <b>NavPolicy(<i>"case"</i>)</b> to find the case and then calls
<b>PlacePolicy()</b> to place the object there.
In addition, our navigation policy is able to take an alternative route to avoid collision with humans as shown in the micro-kitchen demo.
Moreoever, our pick policy is able to change the grasping location if the object moves in the robotics lab demo.
These show that our proposed method is robust to disturbance. -->


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Approach</h2>
        <div class="videoWrapper">
          <!-- <iframe width="100%" height="480"
            src="" title="LSC" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>
          </iframe> -->
          <video id="replay-video"
                 autoplay
                 loop
                 controls
                 muted
                 preload
                 playsinline
                 width="100%">
            <source src="./static/videos/method.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="content has-text-justified">
          <p>
            Natural language provides a simple way for humans to specify a goal for a robot,
            such as <i>"go to the kitchen and bring the sugar to me."</i>
            However, current state-of-the-art language-based embodied AI systems do not have such capability since
            they rely on a fixed-set vocabulary that cannot generalize to diverse instructions.
            In this demo, we propose a method that uses large language models (LLMs) to receive
            a free-form natural language instruction for object rearrangement.
          </p>
          <p>
            The proposed method "Language-guided Skill Coordination" (LSC) consists of three parts:
            (1) an LLM that takes natural language instruction as input and makes calls to a library of skills,
            along with their corresponding skills' input parameters such as the target object name,
            (2) a perception module that stores the location of the receptacles and provides open-vocabulary object detection, and
            (3) a voice-to-text model that processes the audio into text.
          </p>
          <p>
            For example, the top-most video shows a user saying <i>"Bring me the chocolates, cereal and pills to the room table"</i>,
            The LLM takes the text input and makes calls to low-level skills:
            <i>Nav</i>(<i>counter</i>), <i>Pick</i>(<i>cereal</i>), <i>Nav</i>(<i>room table</i>), and <i>Place</i>().
            When executing <i>Nav</i>(<i>counter</i>),
            a perception module is queried to get the location of the counter.
            Then this location is fed into a reinforcement-learning-trained navigation policy
            to let Spot navigate to the target. After Spot finds the counter,
            <i>Pick</i>(<i>cereal</i>) is used to pick up the target object.
            The pick policy is trained to move the arm to the target grasping location of the object with the input of the object bounding box returned by the perception module.
            Finally, Spot uses <i>Nav</i>(<i>room table</i>)</b> to find the room table and then calls
            <i>Place()</i> to place the object there.
            The entire process repeats for rearranging chocolates and pills.
          </p>
          <div class="videoWrapper">
            <!-- <iframe width="100%" height="480"
              src="" title="LSC" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>
            </iframe> -->
            <video id="replay-video"
                   autoplay
                   loop
                   controls
                   muted
                   preload
                   playsinline
                   width="100%">
              <source src="./static/videos/cvpr_video_2.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="videoWrapper">
            <!-- <iframe width="100%" height="480"
              src="" title="LSC" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>
            </iframe> -->
            <video id="replay-video"
                   autoplay
                   loop
                   controls
                   muted
                   preload
                   playsinline
                   width="100%">
              <source src="./static/videos/cvpr_video_3.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="videoWrapper">
            <!-- <iframe width="100%" height="480"
              src="" title="LSC" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>
            </iframe> -->
            <video id="replay-video"
                   autoplay
                   loop
                   controls
                   muted
                   preload
                   playsinline
                   width="100%">
              <source src="./static/videos/cvpr_video_4.mp4"
                      type="video/mp4">
            </video>
          </div>
          <div class="videoWrapper">
            <!-- <iframe width="100%" height="480"
              src="" title="LSC" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen>
            </iframe> -->
            <video id="replay-video"
                   autoplay
                   loop
                   controls
                   muted
                   preload
                   playsinline
                   width="100%">
              <source src="./static/videos/cvpr_video_5.mp4"
                      type="video/mp4">
            </video>
          </div>
          <p>
            In terms of robustness of LSC, the navigation policy is able to take an alternative route to avoid collision with humans.
            In addition, the pick policy is able to change the grasping location if the object moves or the base is not closed enough to the receptacle.
            These show that LSC is robust to disturbance.
          </p>
          <p>
            This demo pieces many efforts together within the Embodied AI team at FAIR.
            For example, one critical component of this demo is the library of skills.
            They are from
            <a href="https://adaptiveskillcoordination.github.io/">Adaptive Skill Coordination for Robotic Mobile Manipulation paper</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Limitations and Future Work</h2>
        <div class="content has-text-justified">
          <p>
            There are several limitations and areas for future work.
            First, while our demo is a reasonably general approach,
            we do not integrate Spot with the skills such as opening doors or drawers.
            This will improve generalization to other mobile manipulation tasks.
            Second, we run our demo in static environments where the location of the object is approximately fixed and the robot is the only agent.
            Future work involves studying human-robot collaboration in a more active way,
            and dynamic environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
